# VL-SurgPT: Bridging Vision and Language for Robust Surgical Point Tracking

This repository contains the official implementation of the paper:

> **VL-SurgPT: Bridging Vision and Language for Robust Surgical Point Tracking**

VL-SurgPT is a multimodal framework for robust tracking of arbitrary points on tissues and instruments in endoscopic surgery videos. It combines vision-language modeling with surgical motion dynamics to improve point tracking and state prediction under realistic conditions.

---

## ğŸ›  Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/VL-SurgPT.git
cd VL-SurgPT
```

### (Optional) Create a virtual environment
```bash
conda create -n vl-surgpt python==3.10
conda activate vl-surgpt
pip install -r requirements.txt
```

## ğŸš€ Inference

### ğŸ”¹ Instrument Point Tracking

Input: Surgical video and initial point prompts on instruments.
Output: Predicted 2D coordinates and motion status per point over time.

To track arbitrary points on surgical instruments and predict their motion status:
```bash
python VL-SurgT-code/code/vis_tracking_status_instrument_update.py
```

### ğŸ”¹ Tissue Point Tracking

To track arbitrary points on tissue surfaces and predict their deformation status:
```bash
python VL-SurgT-code/code/vis_tracking_status_tissue_update.py
```

## ğŸ§  Training
To train VL-SurgPT with text guidance supervision, run:
```bash
python VL-SurgT-code/code/train_text_guidance_V1.py

```


## ğŸ“‚ Dataset Structure
Download address of the dataset:
```bash
https://tinyurl.com/3am5674h

```


```
dataset/
 â”œâ”€â”€ tissue_with_text/
 â”‚   â”œâ”€â”€ 0/  # Deformation
 â”‚       â””â”€â”€ ...
 â”‚   â”œâ”€â”€ 1/  # Instrument Blocking
 â”‚       â””â”€â”€ ...
 â”‚   â”œâ”€â”€ 2/  # Jitter
 â”‚       â””â”€â”€ ...
 â”‚   â”œâ”€â”€ 3/  # Reflection
 â”‚       â””â”€â”€ ...
 â”‚   â””â”€â”€ 4/  # Smoke
 â”‚       â””â”€â”€ left/
 â”‚           â”œâ”€â”€ seq000/
 â”‚           â”‚   â”œâ”€â”€ frames/
 â”‚           â”‚   â”‚   â””â”€â”€ 00000000ms-00001234ms-visible.mp4
 â”‚           â”‚   â””â”€â”€ annotation/
 â”‚           â”‚       â”œâ”€â”€ labels.json         # Point tracking labels
 â”‚           â”‚       â””â”€â”€ texts.json          # Text annotations
 â”‚           â”œâ”€â”€ seq001/
 â”‚           â”‚   â””â”€â”€ ...
 â”‚           â””â”€â”€ seq...
 â””â”€â”€ instrument_with_text/
     â””â”€â”€ 0/
          â””â”€â”€ left/
              â”œâ”€â”€ seq000/
              â”‚   â”œâ”€â”€ frames/
              â”‚   â”‚   â””â”€â”€ 00000000ms-00001234ms-visible.mp4
              â”‚   â””â”€â”€ annotation/
              â”‚       â”œâ”€â”€ labels.json
              â”‚       â””â”€â”€ texts.json
              â”œâ”€â”€ seq001/
              â”‚   â””â”€â”€ ...
              â””â”€â”€ seq...
```

## ğŸ·ï¸ Label Structure

Each frame in the dataset contains two types of annotation:

- 2D coordinates of tracked points (`labels.json`)
- Semantic attributes for each point (`texts.json`)

### ğŸ“`labels.json` File Structure

```
labels.json
â”œâ”€â”€ "0"           # Frame index (as string)
â”‚   â”œâ”€â”€ [x1, y1]  # Coordinates of point 0
â”‚   â”œâ”€â”€ [x2, y2]  # Coordinates of point 1
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ [xN, yN]  # Coordinates of point N or null if obscured
â”œâ”€â”€ "30"
â”‚   â””â”€â”€ [...]
â””â”€â”€ ...
```
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as Tr\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Download checkpoint ===\n",
    "checkpoint_path = \"./checkpoints/track_on_checkpoint.pt\"\n",
    "\n",
    "!wget -O ./checkpoints/track_on_checkpoint.pt \"https://huggingface.co/gaydemir/track_on/resolve/main/track_on_checkpoint.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "def read_video(video_path):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    frames = []\n",
    "    for i, im in enumerate(reader):\n",
    "        frames.append(np.array(im))\n",
    "    video = np.stack(frames)\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2).float()  # (T, 3, 720, 1920)\n",
    "    \n",
    "    print(f\"{video.shape[0]} frames in video\")\n",
    "    \n",
    "    plt.imshow(video[0].permute(1, 2, 0).long())\n",
    "    \n",
    "    return video\n",
    "    \n",
    "def write_gif(png_dir, out_dir):\n",
    "    images = []\n",
    "    \n",
    "    sorted_files = sorted([f for f in os.listdir(png_dir) if f.endswith('.png')], key=lambda x: int(x.split('.')[0]))\n",
    "    \n",
    "    for z, file_name in enumerate(sorted_files):    \n",
    "        file_path = os.path.join(png_dir, file_name)\n",
    "        images.append(imageio.imread(file_path))\n",
    "\n",
    "    imageio.mimsave(out_dir, images, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Read video ====\n",
    "video_path = \"media/messi.mp4\"\n",
    "video = read_video(video_path)  # (T, 3, H, W)\n",
    "# === === ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Set queries manually ====\n",
    "queries = []\n",
    "for x in range(1140, 1200, 20):\n",
    "    for y in range(300, 600, 50):\n",
    "        queries.append([x, y])\n",
    "\n",
    "N = len(queries)\n",
    "\n",
    "distinct_colors = plt.cm.tab20(np.linspace(0, 1, N))\n",
    "hex_colors = ['#%02x%02x%02x' % (int(r*255), int(g*255), int(b*255)) for r, g, b, _ in distinct_colors]\n",
    "\n",
    "queries = torch.tensor(queries)\n",
    "\n",
    "plt.imshow(video[0].permute(1, 2, 0).long())\n",
    "for i, q in enumerate(queries):\n",
    "    plt.scatter(q[0], q[1], s=20, c=hex_colors[i])\n",
    "\n",
    "# === === ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Set Model Arguments ===\n",
    "from utils.train_utils import restart_from_checkpoint_not_dist\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.input_size = [384, 512]\n",
    "\n",
    "        self.N = 384\n",
    "        self.T = 18\n",
    "        self.stride = 4\n",
    "        self.transformer_embedding_dim = 256\n",
    "        self.cnn_corr = False\n",
    "        self.linear_visibility = False\n",
    "        \n",
    "        self.num_layers = 3\n",
    "        self.num_layers_offset_head = 3\n",
    "        \n",
    "        self.num_layers_rerank = 3\n",
    "        self.num_layers_rerank_fusion = 1\n",
    "        self.top_k_regions = 16\n",
    "\n",
    "        self.num_layers_spatial_writer = 3\n",
    "        self.num_layers_spatial_self = 1\n",
    "        self.num_layers_spatial_cross = 1\n",
    "        \n",
    "        self.memory_size = 12\n",
    "        self.val_memory_size = 96\n",
    "        self.val_vis_delta = 0.9\n",
    "        self.random_memory_mask_drop = 0\n",
    "\n",
    "        self.lambda_point = 5.0\n",
    "        self.lambda_vis = 1.0\n",
    "        self.lambda_offset = 1.0\n",
    "        self.lambda_uncertainty = 1.0\n",
    "        self.lambda_top_k = 1.0\n",
    "        \n",
    "        self.epoch_num = 4\n",
    "        self.lr = 1e-3\n",
    "        self.wd = 1e-4\n",
    "        self.bs = 1\n",
    "        self.gradient_acc_steps = 1\n",
    "\n",
    "        self.validation = False\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.seed = 1234\n",
    "        self.loss_after_query = True\n",
    "\n",
    "        self.gpus = torch.cuda.device_count()\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Frame inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model.track_on_ff import TrackOnFF    # Frame Inputs\n",
    "\n",
    "model = TrackOnFF(args)\n",
    "restart_from_checkpoint_not_dist(args, run_variables={}, model=model)\n",
    "\n",
    "model.cuda().eval()\n",
    "model.set_memory_size(args.val_memory_size, args.val_memory_size)\n",
    "model.visibility_treshold = args.val_vis_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "T = video.shape[0]\n",
    "N = queries.shape[0]\n",
    "\n",
    "png_folder = \"./out/messi\"\n",
    "Path(png_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vis_all = []\n",
    "point_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(T):\n",
    "    \n",
    "        # === For the first frame, initialize the queries and memories ===\n",
    "        if t == 0:\n",
    "            model.init_queries_and_memory(queries.cuda(), video[t].unsqueeze(0).cuda())\n",
    "        # === === ===\n",
    "    \n",
    "        # === Model forward, for each frame ===\n",
    "        \n",
    "        point, vis = model.ff_forward(video[t].unsqueeze(0).cuda())\n",
    "        # === === ===\n",
    "    \n",
    "        # === Save the predictions frame-by-frame ===\n",
    "        vis_all.append(vis)\n",
    "        point_all.append(point)\n",
    "        \n",
    "        plt.imshow(video[t].permute(1, 2, 0).long())\n",
    "        for n in range(N):\n",
    "            if vis[n]:\n",
    "                plt.scatter(point[n, 0].cpu(), point[n, 1].cpu(), c=hex_colors[n], s=20)\n",
    "    \n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(os.path.join(png_folder, f\"{t}.png\"), bbox_inches='tight')\n",
    "        plt.clf()\n",
    "        # === === ===\n",
    "\n",
    "write_gif(png_folder, os.path.join(png_folder, \"out.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Video inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model.track_on import TrackOn    # Video Inputs\n",
    "\n",
    "model = TrackOn(args)\n",
    "restart_from_checkpoint_not_dist(args, run_variables={}, model=model)\n",
    "\n",
    "model.cuda().eval()\n",
    "model.set_memory_size(args.val_memory_size, args.val_memory_size)\n",
    "model.visibility_treshold = args.val_vis_delta\n",
    "\n",
    "\n",
    "T = video.shape[0]\n",
    "N = queries.shape[0]\n",
    "\n",
    "video_tmp = video.unsqueeze(0).cuda()                                                # (1, T, 3, H, W)\n",
    "queries_tmp = torch.cat([torch.zeros(N, 1, device=queries.device), queries], dim=1)  # (N, 3), to (t, x, y) format, with all t = 0\n",
    "queries_tmp = queries_tmp.unsqueeze(0).cuda()                                        # (1, N, 3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.inference(video_tmp, queries_tmp)\n",
    "    \n",
    "vis_all = out[\"visibility\"]   # (1, T, N)\n",
    "point_all = out[\"points\"]     # (1, T, N, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Save the predictions by looping them all ===\n",
    "png_folder = \"./out/messi\"\n",
    "Path(png_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for t in range(T):\n",
    "    plt.imshow(video_tmp[0, t].permute(1, 2, 0).long().cpu())\n",
    "    for n in range(N):\n",
    "        if vis_all[0, t, n]:\n",
    "            plt.scatter(point_all[0, t, n, 0].cpu(), point_all[0, t, n, 1].cpu(), c=hex_colors[n], s=20)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(png_folder, f\"{t}.png\"), bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "write_gif(png_folder, os.path.join(png_folder, \"out_vi.gif\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
